# -*- coding: utf-8 -*-
"""ECE180FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HtdAJHJlLYcI00PNrg0rUUlrbzWw872r
"""

#!gdown --id '1rb7Nk-T78dXCru09nRlQb-rYYrUvB31V' --output sp500_companies.csv
#!gdown --id '1CKuLIVoB_8Ryo8EIlXwWFp5YIIzMWxdC' --output sp500_index.csv
!gdown --id '1g1s5RF87Fpy326FMzeQaPSMb_lVub1Aj' --output sp500_stocks.csv

#!gdown --id '1H2xbAj0R7r_q0D0FDy4H6yO4Qw11yUoq' --output Goog.csv

import pandas as pd
import numpy as np
import torch

import torch.nn as nn
import torch.optim as optim
import torch.utils.data as torchdata
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from itertools import product
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold

# Load the dataset
file_path = 'sp500_stocks.csv'
df = pd.read_csv(file_path)

symbol = 'Symbol'
date = 'Date'
close= 'Close'

# Ensure that the required columns are in the dataframe
required_columns = [symbol, date, close]
if not all(col in df.columns for col in required_columns):
    raise ValueError("One or more required columns are missing from the dataframe")

# Filter the data for Google
google_df = df[df[symbol] == 'GOOGL']

if google_df.empty:
    raise ValueError("No data found for the symbol 'GOOGL'")

# Sort by date
google_df[date] = pd.to_datetime(google_df[date])
google_df = google_df.sort_values(by=date)

# Remove rows with missing values
google_df = google_df.dropna(subset=[close])

if google_df.empty:
    raise ValueError("All rows with missing required values have been dropped, resulting in an empty dataframe")

# Extract the relevant columns
data = google_df[[date, close]].copy()

# Normalize the columns
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_columns = data[[close]]
scaled_data = scaler.fit_transform(scaled_columns)

# Combine the dates with the scaled data
scaled_data_df = pd.DataFrame(scaled_data, columns=[close])
scaled_data_df.insert(0, date, data[date].values)

dates = data[date].to_numpy()

# Create sequences for LSTM
def create_dataset(dataset, lookback, forecast_horizon):
    X, y = [], []
    for i in range(len(dataset) - lookback - forecast_horizon + 1):
        X.append(dataset[i:i + lookback])
        y.append(dataset[i + lookback : i + lookback + forecast_horizon, 0])  # Only take 'close' price for y
    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

train_size = int(len(scaled_data) * 0.80)
val_size = int(len(scaled_data) * 0.1)

# Move tensors to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_layer_size=40, num_layers=1, output_size=10, dropout=0.125):
        super(LSTMModel, self).__init__()
        self.hidden_layer_size = hidden_layer_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size, hidden_layer_size, 1, batch_first=True, bidirectional=True)
        self.linear1 = nn.Linear(hidden_layer_size * 2, hidden_layer_size)
        self.linear2 = nn.Linear(hidden_layer_size, 50)
        self.linear3 = nn.Linear(50, output_size)  # Output size for forecast horizon
        self.dropout = nn.Dropout(p=dropout)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.to(self.device)

    def forward(self, input_seq):
        batch_size = input_seq.size(0)
        hidden_cell = (torch.zeros(self.num_layers * 2, batch_size, self.hidden_layer_size).to(self.device),
                       torch.zeros(self.num_layers * 2, batch_size, self.hidden_layer_size).to(self.device))

        lstm_out, hidden_cell = self.lstm(input_seq, hidden_cell)
        x = self.dropout(lstm_out[:, -1, :])
        x = torch.nn.functional.elu(self.linear1(x))
        x = torch.nn.functional.elu(self.linear2(x))
        predictions = self.linear3(x)  # This will output (batch_size, forecast_horizon)
        return predictions

# Define the custom weighted MSE loss function
class WeightedMSELoss(nn.Module):
    def __init__(self, weights):
        super(WeightedMSELoss, self).__init__()
        self.weights = torch.tensor(weights, dtype=torch.float32).to(device)

    def forward(self, y_pred, y_true):
        return torch.mean(self.weights * (y_pred - y_true) ** 2)

# Hyperparameters
learning_rates = [0.001]
hidden_layer_sizes = [40]
dropout_ratios = [0.125]
lookbacks = [100]
forecast_horizon = 10


best_val_loss = float('inf')
best_train_losses = []
best_val_losses = []
best_val_r2_history = []
best_test_r2_history = []
best_val_r2loss_history = []
best_test_r2loss_history = []

best_lr = None
best_hl = None
best_do = None
best_lb = None
best_model = None

epochs = 300
patience = 25

weights = np.linspace(1, 0.1, forecast_horizon)  # Linearly decreasing weights

# Define the sizes
total_size = len(scaled_data)
test_size = total_size // 10
val_size = total_size // 10
train_size = total_size - test_size - val_size

# Split into train and test sets first
train_val_data = scaled_data[:train_size]
test_data = scaled_data[train_size:]

train_val_dates = dates[:train_size]
test_dates = dates[train_size:]

n_splits = 5

kf = KFold(n_splits = n_splits)

for lr, hl, do, lb in product(learning_rates, hidden_layer_sizes, dropout_ratios, lookbacks):
    print(f"Learning rate {lr}: Dropout rate {do}: Hidden layer size {hl}: Lookback {lb}")

    fold_final_val_losses = []
    fold_val_r2_scores = []
    n = n_splits

    for train_index, val_index in kf.split(train_val_data):
        if(n == 1):
            break
        else:
            n -= 1
        train_fold_data = train_val_data[train_index]
        val_fold_data = train_val_data[val_index]

        X_train, y_train = create_dataset(train_fold_data, lookback=lb, forecast_horizon=forecast_horizon)
        X_val, y_val = create_dataset(val_fold_data, lookback=lb, forecast_horizon=forecast_horizon)
        X_test, y_test = create_dataset(test_data, lookback=lb, forecast_horizon=forecast_horizon)

        # Move tensors to GPU if available
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        train_loader = torchdata.DataLoader(torchdata.TensorDataset(X_train, y_train), batch_size=forecast_horizon, drop_last=True)
        val_loader = torchdata.DataLoader(torchdata.TensorDataset(X_val, y_val), batch_size=forecast_horizon, drop_last=True)

        # Initialize the model, loss function, and optimizer
        model = LSTMModel(hidden_layer_size=hl, dropout=do, output_size=forecast_horizon).to(device)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        loss_function = WeightedMSELoss(weights)
        optimizer = optim.Adam(model.parameters(), lr=lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

        train_losses = []
        val_losses = []
        val_r2_history = []
        test_r2_history = []
        val_r2loss_history = []
        test_r2loss_history = []

        patience_counter = 0
        best_current_val_loss = float('inf')
        for epoch in range(epochs):
            model.train()
            train_batch_losses = []
            val_batch_losses = []
            val_r2_batch = []
            test_r2_batch = []
            val_r2loss_batch = []
            test_r2loss_batch = []
            for X_batch, y_batch in train_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()
                y_pred = model(X_batch)
                loss = loss_function(y_pred, y_batch)
                loss.backward()
                optimizer.step()
                train_batch_losses.append(loss.item())

            train_loss = np.mean(train_batch_losses)
            train_losses.append(train_loss)

            model.eval()
            with torch.no_grad():
                val_plot = np.empty((len(scaled_data), forecast_horizon))
                val_plot[:, :] = np.nan
                val_pred = model(X_val.to(device)).cpu().numpy()
                for i in range(len(val_pred)):
                    val_plot[train_size + i: train_size + i + forecast_horizon] = val_pred[i]
                for day in range(forecast_horizon):
                    val_r2loss_batch.append((1 - r2_score(y_val[:, day], val_pred[:, day])))
                    val_r2_batch.append(r2_score(y_val[:, day], val_pred[:, day]))
                val_r2_score = np.mean(weights.dot(val_r2loss_batch))
                val_r2loss_history.append(val_r2_score)
                val_r2_history.append(val_r2_batch)

                test_plot = np.empty((len(scaled_data), forecast_horizon))
                test_plot[:, :] = np.nan
                test_pred = model(X_test.to(device)).cpu().numpy()
                for i in range(len(test_pred)):
                    test_plot[train_size + val_size + i: train_size + val_size + i + forecast_horizon] = test_pred[i]
                for day in range(forecast_horizon):
                    test_r2loss_batch.append((1 - r2_score(y_test[:, day], test_pred[:, day])))
                    test_r2_batch.append(r2_score(y_test[:, day], test_pred[:, day]))
                test_r2_score = np.mean(weights.dot(test_r2loss_batch))
                test_r2loss_history.append(test_r2_score)
                test_r2_history.append(test_r2_batch)

                for X_batch, y_batch in val_loader:
                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                    y_pred = model(X_batch)
                    loss = loss_function(y_pred, y_batch)
                    val_batch_losses.append(loss.item())
                val_loss = np.mean(val_batch_losses)
                val_losses.append(val_loss)

            scheduler.step(val_loss)

            #if epoch % 10 == 0:
                #print(f"Epoch {epoch}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}, Val R2 score {val_r2_score:.4f}, test R2 score {test_r2_score:.4f}, day 1,2,5,10 val r2 {val_r2_batch[0]:.3f} {val_r2_batch[1]:.3f} {val_r2_batch[4]:.3f}, day 1,2,5,10 test r2 {test_r2_batch[0]:.3f} {test_r2_batch[1]:.3f} {test_r2_batch[4]:.3f}")

            if val_loss < best_current_val_loss*0.999:
                best_current_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= patience:
                #print(f"Early stopping at epoch {epoch}")
                break

        fold_final_val_losses.append(val_losses[-1])
        fold_val_r2_scores.append(val_r2loss_history[-1])

    mean_val_loss = np.mean(fold_final_val_losses)
    mean_val_r2_score = np.mean(fold_val_r2_scores)

    print(f"Mean Validation Loss: {mean_val_loss:.7f}, Mean Validation R2 Score: {mean_val_r2_score:.4f}")

    if mean_val_loss < best_val_loss:
        best_lr = lr
        best_do = do
        best_hl = hl
        best_lb = lb
        best_val_loss = mean_val_loss
        best_model = model
        best_train_losses = train_losses
        best_val_losses = val_losses
        best_test_r2_history = test_r2_history
        best_val_r2_history = val_r2_history
        best_val_r2loss_history = val_r2loss_history
        best_test_r2loss_history = test_r2loss_history

print(f"Learning rate {best_lr}: Dropout rate {best_do}: Hidden layer size {best_hl}: Lookback {best_lb}, Best Val RMSE {best_val_loss:.6f}")

# Plotting the training loss and validation RMSE
plt.figure(figsize=(10, 6))
plt.plot(best_train_losses, label='Training Loss')
plt.plot(best_val_losses, label='Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('Loss / RMSE')
plt.yscale('log')
plt.legend()
plt.show()

# Plotting the r2 score for validation and test
plt.figure(figsize=(10, 6))
plt.plot(best_val_r2_history, label='Val R2')
plt.xlabel('Epoch')
plt.ylabel('R2')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(best_test_r2_history, label='Test R2')
plt.xlabel('Epoch')
plt.ylabel('R2')
plt.legend()
plt.show()

# Prepare the data for training, validation, and testing
train_data, test_data, val_data = scaled_data[:train_size + forecast_horizon], scaled_data[train_size + val_size - best_lb:], scaled_data[train_size - best_lb : train_size + val_size + forecast_horizon]
train_dates, test_dates, val_dates = dates[:train_size], dates[train_size + val_size - best_lb:], dates[train_size - best_lb: train_size + val_size]
X_train, y_train = create_dataset(train_data, lookback=best_lb, forecast_horizon=forecast_horizon)
X_test, y_test = create_dataset(test_data, lookback=best_lb, forecast_horizon=forecast_horizon)
X_val, y_val = create_dataset(val_data, lookback=best_lb, forecast_horizon=forecast_horizon)

with torch.no_grad():
    # Create arrays to hold predictions
    train_plot = np.empty(shape = (len(dates), 10))
    train_plot[:, :] = np.nan
    val_plot = np.empty(shape = (len(dates), 10))
    val_plot[:, :] = np.nan
    test_plot = np.empty(shape = (len(dates), 10))
    test_plot[:, :] = np.nan

    # Generate predictions for training, validation, and test sets
    train_pred = best_model(X_train.to(device)).cpu().numpy()
    val_pred = best_model(X_val.to(device)).cpu().numpy()
    test_pred = best_model(X_test.to(device)).cpu().numpy()

    # Align predictions with corresponding dates
    for i in range(len(train_pred)):
        train_plot[best_lb + i : best_lb + i + forecast_horizon] = train_pred[i]
    for i in range(len(val_pred)):
        val_plot[train_size + i : train_size + i + forecast_horizon] = val_pred[i]
    for i in range(len(test_pred)):
        test_plot[train_size + val_size + i : train_size + val_size + i + forecast_horizon] = test_pred[i]

    # Inverse transform to original scale
    #train_plot  = np.tile(train_plot .reshape(10, len(dates)), (10, 5))
    #train_plot  = scaler.inverse_transform(train_plot )[:, 0]
    #train_plot = scaler.inverse_transform(train_plot)
    #val_plot = scaler.inverse_transform(val_plot)
    #test_plot = scaler.inverse_transform(test_plot)
    #actual_plot = scaler.inverse_transform(scaled_data)[:, 0]
    actual_plot = scaled_data[:, 0]

# Plot the results (overall)
plt.figure(figsize=(10, 6))
plt.plot(dates, actual_plot, label='Actual Prices')
plt.plot(dates, train_plot[:, 0], c='r', label='Train Predictions')
plt.plot(dates, val_plot[:, 0], c='g', label='Val Predictions')
plt.plot(dates, test_plot[:, 0], c='orange', label='Test Predictions')
plt.xlabel('Date')
plt.ylabel('Google Stock Price (Normalized)')
plt.legend()
plt.show()

# Plot the results (val only)
plt.figure(figsize=(10, 6))
val_start_idx = train_size
val_end_idx = val_start_idx + val_size
plt.plot(dates[val_start_idx:val_end_idx], actual_plot[val_start_idx:val_end_idx], label='Actual Prices')
plt.plot(dates[val_start_idx - 1:val_end_idx - 1], val_plot[val_start_idx:val_end_idx, 0], c='green', label='Val Predictions')
plt.xlabel('Date')
plt.ylabel('Google Stock Price (Normalized)')
plt.legend()
plt.show()

# Plot the results (test only)
plt.figure(figsize=(10, 6))
test_start_idx = train_size + val_size
test_end_idx = test_start_idx + len(test_plot[test_start_idx:]) - forecast_horizon
plt.plot(dates[test_start_idx:test_end_idx], actual_plot[test_start_idx:test_end_idx], label='Actual Prices')
plt.plot(dates[test_start_idx - 1:test_end_idx - 1], test_plot[test_start_idx:test_end_idx, 0], c='orange', label='Test Predictions')
plt.xlabel('Date')
plt.ylabel('Google Stock Price (Normalized)')
plt.legend()
plt.show()

# Plot the results (test only)
plt.figure(figsize=(10, 6))
test_start_idx = train_size + val_size
test_end_idx = test_start_idx + len(test_plot[test_start_idx:]) - forecast_horizon
plt.plot(dates[test_start_idx:test_end_idx], actual_plot[test_start_idx:test_end_idx], label='Actual Prices')
plt.plot(dates[test_start_idx - 1:test_end_idx - 1], test_plot[test_start_idx:test_end_idx, 9], c='orange', label='Test Predictions')
plt.xlabel('Date')
plt.ylabel('Google Stock Price (Normalized)')
plt.legend()
plt.show()

# Calculate R² scores for each day in the forecast horizon
for day in range(forecast_horizon):
    r2_score_day = r2_score(y_test[:, day], test_pred[:, day])
    print(f'R² score for day {day + 1}: {r2_score_day:.4f}')

# Combine the entire dataset for training
full_data = scaled_data[-forecast_horizon:]
full_dates = dates[:]

# Prepare the entire dataset
X_full, y_full = create_dataset(full_data, lookback=best_lb, forecast_horizon=forecast_horizon)

# Move tensors to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
full_loader = torchdata.DataLoader(torchdata.TensorDataset(X_full, y_full), batch_size=forecast_horizon, drop_last=True)

# Initialize the model with the best hyperparameters
final_model = best_model
torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)
loss_function = WeightedMSELoss(weights)
optimizer = optim.Adam(final_model.parameters(), lr=best_lr)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

final_train_losses = []
final_val_losses = []
final_val_r2_history = []
final_test_r2_history = []
final_val_r2loss_history = []
final_test_r2loss_history = []

patience = 15

patience_counter = 0
best_current_loss = float('inf')
for epoch in range(epochs):
    final_model.train()
    train_batch_losses = []
    for X_batch, y_batch in full_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        y_pred = final_model(X_batch)
        loss = loss_function(y_pred, y_batch)
        loss.backward()
        optimizer.step()
        train_batch_losses.append(loss.item())

    train_loss = np.sum(train_batch_losses)
    final_train_losses.append(train_loss)

    scheduler.step(train_loss)

    if epoch % 10 == 0:
        print(f"Epoch {epoch}: Train Loss {train_loss:.4f}")

    if train_loss < best_current_loss * 0.99:
        best_current_loss = train_loss
        patience_counter = 0
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch}")
        break

# Save the final model
torch.save(final_model.state_dict(), "best_model_trained_on_full_data.pth")

print("Training complete. Best model saved.")

# Extended prediction beyond the dataset
def predict_future(model, data, lookback, forecast_horizon, steps):
    predictions = []
    current_seq = data[-lookback:].reshape(1, lookback, 5)
    current_seq = torch.tensor(current_seq, dtype=torch.float32).to(device)

    model.eval()
    for _ in range(steps):
        with torch.no_grad():
            pred = model(current_seq).cpu().numpy()
            predictions.append(pred.flatten())
            new_seq = np.append(current_seq.cpu().numpy().flatten(), pred.flatten())[-lookback:]
            current_seq = torch.tensor(new_seq.reshape(1, lookback, 1), dtype=torch.float32).to(device)

    return np.array(predictions).reshape(-1, forecast_horizon)

# Number of future steps to predict
future_steps = 1 # MUST BE 1 FOR CURRENT VERSION
future_predictions = predict_future(final_model, scaled_data[:-forecast_horizon], best_lb, forecast_horizon, future_steps)

# Generate future dates
last_date = dates[-1]
future_dates = pd.date_range(start=last_date, periods=(future_steps * forecast_horizon + 1))[1:]

future_predictions = np.tile(future_predictions.reshape(10, 1), (10, 5))
# Inverse transform future predictions
future_predictions = scaler.inverse_transform(future_predictions)[:, 0]

# Plot extended predictions
plt.figure(figsize=(10, 6))
#plt.plot(future_dates.to_numpy(), future_predictions.flatten()[:10], label='Future Predictions', linestyle='--')
plt.plot(dates[-forecast_horizon:], future_predictions.flatten()[:10], label='Predictions', linestyle='--')
plt.plot(dates[-forecast_horizon:], data[close].to_numpy()[-forecast_horizon:], label='Actual', linestyle='--')
plt.xlabel('Date')
plt.ylabel('Google Stock Price')
plt.legend()
plt.show()